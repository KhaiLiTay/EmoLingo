{"cells":[{"cell_type":"markdown","metadata":{"id":"2xRT-tWZmoq-"},"source":["Mounted Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50560,"status":"ok","timestamp":1733310343329,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"QV_gkGJ_mrZG","outputId":"5fd0c29f-f9c3-4989-83b7-b188ce8001ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"_DMNRGWSnXrn"},"source":["import package"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5479,"status":"ok","timestamp":1733310348805,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"9BEwhwUEc7XF"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":97025,"status":"ok","timestamp":1733310445825,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"ZLczQcEksz76","outputId":"d5be9f26-b03a-448f-f8c9-c25dd89adbd8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Collecting speechrecognition\n","  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers\u003c0.21,\u003e=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch) (1.3.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.4.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.2.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (3.0.2)\n","Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: speechrecognition\n","Successfully installed speechrecognition-3.11.0\n","Collecting TTS\n","  Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl.metadata (21 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: cython\u003e=0.29.30 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.11)\n","Requirement already satisfied: scipy\u003e=1.11.2 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.13.1)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS) (2.5.1+cu121)\n","Requirement already satisfied: soundfile\u003e=0.12.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.12.1)\n","Requirement already satisfied: librosa\u003e=0.10.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.2.post1)\n","Requirement already satisfied: scikit-learn\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.5.2)\n","Requirement already satisfied: inflect\u003e=5.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (7.4.0)\n","Requirement already satisfied: tqdm\u003e=4.64.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.66.6)\n","Collecting anyascii\u003e=0.3.0 (from TTS)\n","  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: pyyaml\u003e=6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (6.0.2)\n","Requirement already satisfied: fsspec\u003e=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2024.10.0)\n","Requirement already satisfied: aiohttp\u003e=3.8.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.11.2)\n","Requirement already satisfied: packaging\u003e=23.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (24.2)\n","Requirement already satisfied: flask\u003e=2.0.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.3)\n","Collecting pysbd\u003e=0.3.4 (from TTS)\n","  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n","Collecting umap-learn\u003e=0.5.1 (from TTS)\n","  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n","Collecting pandas\u003c2.0,\u003e=1.4 (from TTS)\n","  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: matplotlib\u003e=3.7.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.0)\n","Collecting trainer\u003e=0.0.32 (from TTS)\n","  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n","Collecting coqpit\u003e=0.0.16 (from TTS)\n","  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n","Collecting pypinyin (from TTS)\n","  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n","Collecting hangul-romanize (from TTS)\n","  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading gruut-2.2.3.tar.gz (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jamo (from TTS)\n","  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.9.1)\n","Collecting g2pkk\u003e=0.1.1 (from TTS)\n","  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Collecting bangla (from TTS)\n","  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n","Collecting bnnumerizer (from TTS)\n","  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bnunicodenormalizer (from TTS)\n","  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: einops\u003e=0.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.8.0)\n","Requirement already satisfied: transformers\u003e=4.33.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.46.2)\n","Collecting encodec\u003e=0.1.1 (from TTS)\n","  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting unidecode\u003e=1.3.2 (from TTS)\n","  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n","Collecting num2words (from TTS)\n","  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: spacy\u003e=3 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]\u003e=3-\u003eTTS) (3.7.5)\n","Collecting numpy==1.22.0 (from TTS)\n","  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n","Requirement already satisfied: numba\u003e=0.57.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.60.0)\n","Requirement already satisfied: Babel\u003c3.0.0,\u003e=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS) (2.16.0)\n","Collecting dateparser~=1.1.0 (from gruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n","Collecting gruut-ipa\u003c1.0,\u003e=0.12.0 (from gruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jsonlines~=1.2.0 (from gruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting networkx\u003c3.0.0,\u003e=2.5.0 (from gruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n","Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3-\u003eTTS)\n","  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003e=3.8.1-\u003eTTS) (2.4.3)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003e=3.8.1-\u003eTTS) (1.3.1)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003e=3.8.1-\u003eTTS) (24.2.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003e=3.8.1-\u003eTTS) (1.5.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003e=3.8.1-\u003eTTS) (6.1.0)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003e=3.8.1-\u003eTTS) (0.2.0)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003e=3.8.1-\u003eTTS) (1.17.2)\n","Requirement already satisfied: async-timeout\u003c6.0,\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003e=3.8.1-\u003eTTS) (4.0.3)\n","Requirement already satisfied: Werkzeug\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask\u003e=2.0.1-\u003eTTS) (3.1.3)\n","Requirement already satisfied: itsdangerous\u003e=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask\u003e=2.0.1-\u003eTTS) (2.2.0)\n","Requirement already satisfied: click\u003e=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask\u003e=2.0.1-\u003eTTS) (8.1.7)\n","Requirement already satisfied: blinker\u003e=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask\u003e=2.0.1-\u003eTTS) (1.9.0)\n","Requirement already satisfied: more-itertools\u003e=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect\u003e=5.6.0-\u003eTTS) (10.5.0)\n","Requirement already satisfied: typeguard\u003e=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect\u003e=5.6.0-\u003eTTS) (4.4.1)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (3.0.2)\n","Requirement already satisfied: audioread\u003e=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa\u003e=0.10.0-\u003eTTS) (3.0.1)\n","INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n","Collecting librosa\u003e=0.10.0 (from TTS)\n","  Downloading librosa-0.10.2-py3-none-any.whl.metadata (8.6 kB)\n","  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n","  Downloading librosa-0.10.0.post2-py3-none-any.whl.metadata (8.3 kB)\n","  Downloading librosa-0.10.0.post1-py3-none-any.whl.metadata (8.3 kB)\n","  Downloading librosa-0.10.0-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: joblib\u003e=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa\u003e=0.10.0-\u003eTTS) (1.4.2)\n","Requirement already satisfied: decorator\u003e=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa\u003e=0.10.0-\u003eTTS) (4.4.2)\n","Requirement already satisfied: pooch\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa\u003e=0.10.0-\u003eTTS) (1.8.2)\n","Requirement already satisfied: soxr\u003e=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa\u003e=0.10.0-\u003eTTS) (0.5.0.post1)\n","Requirement already satisfied: lazy-loader\u003e=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa\u003e=0.10.0-\u003eTTS) (0.4)\n","Requirement already satisfied: msgpack\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa\u003e=0.10.0-\u003eTTS) (1.1.0)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.7.0-\u003eTTS) (1.3.1)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.7.0-\u003eTTS) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.7.0-\u003eTTS) (4.55.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.7.0-\u003eTTS) (1.4.7)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.7.0-\u003eTTS) (11.0.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.7.0-\u003eTTS) (3.2.0)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.7.0-\u003eTTS) (2.8.2)\n","Collecting docopt\u003e=0.6.2 (from num2words-\u003eTTS)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: llvmlite\u003c0.44,\u003e=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba\u003e=0.57.0-\u003eTTS) (0.43.0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas\u003c2.0,\u003e=1.4-\u003eTTS) (2024.2)\n","Requirement already satisfied: threadpoolctl\u003e=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn\u003e=1.3.0-\u003eTTS) (3.5.0)\n","INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n","Collecting scipy\u003e=1.11.2 (from TTS)\n","  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cffi\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile\u003e=0.12.0-\u003eTTS) (1.17.1)\n","Requirement already satisfied: spacy-legacy\u003c3.1.0,\u003e=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (3.0.12)\n","Requirement already satisfied: spacy-loggers\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (1.0.5)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (1.0.10)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2.0.8)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (3.0.9)\n","Requirement already satisfied: thinc\u003c8.3.0,\u003e=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (8.2.5)\n","Requirement already satisfied: wasabi\u003c1.2.0,\u003e=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (1.1.3)\n","Requirement already satisfied: srsly\u003c3.0.0,\u003e=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2.4.8)\n","Requirement already satisfied: catalogue\u003c2.1.0,\u003e=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2.0.10)\n","Requirement already satisfied: weasel\u003c0.5.0,\u003e=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (0.4.1)\n","Requirement already satisfied: typer\u003c1.0.0,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (0.13.0)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2.9.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (75.1.0)\n","Requirement already satisfied: langcodes\u003c4.0.0,\u003e=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (3.4.1)\n","Collecting sudachipy!=0.6.1,\u003e=0.5.2 (from spacy[ja]\u003e=3-\u003eTTS)\n","  Downloading SudachiPy-0.6.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting sudachidict-core\u003e=20211220 (from spacy[ja]\u003e=3-\u003eTTS)\n","  Downloading SudachiDict_core-20241021-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer\u003e=0.0.32-\u003eTTS) (5.9.5)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from trainer\u003e=0.0.32-\u003eTTS) (2.17.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.33.0-\u003eTTS) (0.26.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.33.0-\u003eTTS) (2024.9.11)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.33.0-\u003eTTS) (0.4.5)\n","Requirement already satisfied: tokenizers\u003c0.21,\u003e=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.33.0-\u003eTTS) (0.20.3)\n","Collecting pynndescent\u003e=0.5 (from umap-learn\u003e=0.5.1-\u003eTTS)\n","  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi\u003e=1.0-\u003esoundfile\u003e=0.12.0-\u003eTTS) (2.22)\n","INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\n","Collecting contourpy\u003e=1.0.1 (from matplotlib\u003e=3.7.0-\u003eTTS)\n","  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n","  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0-\u003egruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS) (5.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0-\u003egruut==2.2.3-\u003egruut[de,es,fr]==2.2.3-\u003eTTS) (1.16.0)\n","Requirement already satisfied: language-data\u003e=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes\u003c4.0.0,\u003e=3.2.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (1.2.0)\n","Requirement already satisfied: platformdirs\u003e=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch\u003e=1.0-\u003elibrosa\u003e=0.10.0-\u003eTTS) (4.3.6)\n","Requirement already satisfied: annotated-types\u003e=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2.23.4)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (3.4.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2.2.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2024.8.30)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc\u003c8.3.0,\u003e=8.2.2-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (0.7.11)\n","Requirement already satisfied: confection\u003c1.0.0,\u003e=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc\u003c8.3.0,\u003e=8.2.2-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (0.1.5)\n","Requirement already satisfied: shellingham\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (1.5.4)\n","Requirement already satisfied: rich\u003e=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (13.9.4)\n","Requirement already satisfied: cloudpathlib\u003c1.0.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (0.20.0)\n","Requirement already satisfied: smart-open\u003c8.0.0,\u003e=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (7.0.5)\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard-\u003etrainer\u003e=0.0.32-\u003eTTS) (1.4.0)\n","Requirement already satisfied: grpcio\u003e=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard-\u003etrainer\u003e=0.0.32-\u003eTTS) (1.68.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard-\u003etrainer\u003e=0.0.32-\u003eTTS) (3.7)\n","Requirement already satisfied: protobuf!=4.24.0,\u003e=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard-\u003etrainer\u003e=0.0.32-\u003eTTS) (4.25.5)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard-\u003etrainer\u003e=0.0.32-\u003eTTS) (0.7.2)\n","Requirement already satisfied: marisa-trie\u003e=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data\u003e=1.2-\u003elangcodes\u003c4.0.0,\u003e=3.2.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (1.2.1)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (3.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open\u003c8.0.0,\u003e=5.2.1-\u003eweasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003e=3-\u003espacy[ja]\u003e=3-\u003eTTS) (0.1.2)\n","Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl (938 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n","Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n","Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading num2words-0.5.13-py3-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n","Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n","Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n","Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n","Downloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n","Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading SudachiDict_core-20241021-py3-none-any.whl (72.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading SudachiPy-0.6.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n","  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75788 sha256=12a0efd34cee0c664e5b1df8f8af4e6c03494a4ab492752a0e5dddf653169aa5\n","  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n","  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=4259bbbc0dd9c80c289045bf49c18d1a5f8dced8387ca51341918eb2cfdf6f00\n","  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n","  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5261 sha256=f501c6419f15d28ba2aa8327bae8e694a616413cb8644f5170d1e304acc879b1\n","  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=a95e8ca407279d2f694c8a7809e91b03d1ada2156682ffc3a0ad365f193fa11e\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=7af1a43151bfcdb41982ca87e4924690f07b0cadbc72aeec36b13f9fbb6f77f6\n","  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n","  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498313 sha256=09b7dd02afdca5ea37bd513a132c7dcf7b457741f5e792b030495d226625e0c2\n","  Stored in directory: /root/.cache/pip/wheels/83/80/5f/775b357ae61d7cb68793327c7470d848715cbc60bb373af8dd\n","  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326857 sha256=cdb897ba37196ee066fa7676e1770a92b9072ebf96338b97335d69c46f586222\n","  Stored in directory: /root/.cache/pip/wheels/64/8d/b7/d484d224facd899ed188e00374f25dd3f19d1a3f53da6517bd\n","  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173928 sha256=6a2fe527f56d6db6e6a86afaa089322003923c9684423636c3f816f9ba60bf2d\n","  Stored in directory: /root/.cache/pip/wheels/ab/bd/96/5ddde14e8e6932a96f12c5ab5de62b619d39e2507d7daf5188\n","  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968766 sha256=24707eba455ed3df1579f37e3e801390bafa50c2e70a77892f678350b945c71e\n","  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\n","Successfully built gruut encodec bnnumerizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n","Installing collected packages: sudachipy, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, python-crfsuite, pysbd, pypinyin, numpy, num2words, networkx, jsonlines, gruut-ipa, coqpit, anyascii, scipy, pandas, g2pkk, dateparser, contourpy, trainer, gruut, pynndescent, librosa, encodec, umap-learn, TTS\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.4.2\n","    Uninstalling networkx-3.4.2:\n","      Successfully uninstalled networkx-3.4.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.13.1\n","    Uninstalling scipy-1.13.1:\n","      Successfully uninstalled scipy-1.13.1\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: contourpy\n","    Found existing installation: contourpy 1.3.1\n","    Uninstalling contourpy-1.3.1:\n","      Successfully uninstalled contourpy-1.3.1\n","  Attempting uninstall: librosa\n","    Found existing installation: librosa 0.10.2.post1\n","    Uninstalling librosa-0.10.2.post1:\n","      Successfully uninstalled librosa-0.10.2.post1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albucore 0.0.19 requires numpy\u003e=1.24.4, but you have numpy 1.22.0 which is incompatible.\n","albumentations 1.4.20 requires numpy\u003e=1.24.4, but you have numpy 1.22.0 which is incompatible.\n","arviz 0.20.0 requires numpy\u003e=1.23.0, but you have numpy 1.22.0 which is incompatible.\n","astropy 6.1.6 requires numpy\u003e=1.23, but you have numpy 1.22.0 which is incompatible.\n","bigframes 1.27.0 requires numpy\u003e=1.24.0, but you have numpy 1.22.0 which is incompatible.\n","chex 0.1.87 requires numpy\u003e=1.24.1, but you have numpy 1.22.0 which is incompatible.\n","cudf-cu12 24.10.1 requires numpy\u003c3.0a0,\u003e=1.23, but you have numpy 1.22.0 which is incompatible.\n","cudf-cu12 24.10.1 requires pandas\u003c2.2.3dev0,\u003e=2.0, but you have pandas 1.5.3 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n","ibis-framework 9.2.0 requires numpy\u003c3,\u003e=1.23.2, but you have numpy 1.22.0 which is incompatible.\n","jax 0.4.33 requires numpy\u003e=1.24, but you have numpy 1.22.0 which is incompatible.\n","jaxlib 0.4.33 requires numpy\u003e=1.24, but you have numpy 1.22.0 which is incompatible.\n","mizani 0.13.0 requires numpy\u003e=1.23.5, but you have numpy 1.22.0 which is incompatible.\n","mizani 0.13.0 requires pandas\u003e=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","numexpr 2.10.1 requires numpy\u003e=1.23.0, but you have numpy 1.22.0 which is incompatible.\n","nx-cugraph-cu12 24.10.0 requires networkx\u003e=3.0, but you have networkx 2.8.8 which is incompatible.\n","nx-cugraph-cu12 24.10.0 requires numpy\u003c3.0a0,\u003e=1.23, but you have numpy 1.22.0 which is incompatible.\n","pandas-stubs 2.2.2.240909 requires numpy\u003e=1.23.5, but you have numpy 1.22.0 which is incompatible.\n","plotnine 0.14.1 requires numpy\u003e=1.23.5, but you have numpy 1.22.0 which is incompatible.\n","plotnine 0.14.1 requires pandas\u003e=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","pylibraft-cu12 24.10.0 requires numpy\u003c3.0a0,\u003e=1.23, but you have numpy 1.22.0 which is incompatible.\n","rmm-cu12 24.10.0 requires numpy\u003c3.0a0,\u003e=1.23, but you have numpy 1.22.0 which is incompatible.\n","scikit-image 0.24.0 requires numpy\u003e=1.23, but you have numpy 1.22.0 which is incompatible.\n","statsmodels 0.14.4 requires numpy\u003c3,\u003e=1.22.3, but you have numpy 1.22.0 which is incompatible.\n","tensorflow 2.17.1 requires numpy\u003c2.0.0,\u003e=1.23.5; python_version \u003c= \"3.11\", but you have numpy 1.22.0 which is incompatible.\n","xarray 2024.10.0 requires numpy\u003e=1.24, but you have numpy 1.22.0 which is incompatible.\n","xarray 2024.10.0 requires pandas\u003e=2.1, but you have pandas 1.5.3 which is incompatible.\n","xarray-einstats 0.8.0 requires numpy\u003e=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 contourpy-1.2.1 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 librosa-0.10.0 networkx-2.8.8 num2words-0.5.13 numpy-1.22.0 pandas-1.5.3 pynndescent-0.5.13 pypinyin-0.53.0 pysbd-0.3.4 python-crfsuite-0.9.11 scipy-1.11.4 sudachidict-core-20241021 sudachipy-0.6.9 trainer-0.0.36 umap-learn-0.5.7 unidecode-1.3.8\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"0280c8be22e1484e9ad21dda675c6a01","pip_warning":{"packages":["numpy","pandas"]}}},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Collecting numpy==1.23.5\n","  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.0\n","    Uninstalling numpy-1.22.0:\n","      Successfully uninstalled numpy-1.22.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tts 0.22.0 requires numpy==1.22.0; python_version \u003c= \"3.10\", but you have numpy 1.23.5 which is incompatible.\n","albucore 0.0.19 requires numpy\u003e=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","albumentations 1.4.20 requires numpy\u003e=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","bigframes 1.27.0 requires numpy\u003e=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","chex 0.1.87 requires numpy\u003e=1.24.1, but you have numpy 1.23.5 which is incompatible.\n","cudf-cu12 24.10.1 requires pandas\u003c2.2.3dev0,\u003e=2.0, but you have pandas 1.5.3 which is incompatible.\n","jax 0.4.33 requires numpy\u003e=1.24, but you have numpy 1.23.5 which is incompatible.\n","jaxlib 0.4.33 requires numpy\u003e=1.24, but you have numpy 1.23.5 which is incompatible.\n","mizani 0.13.0 requires pandas\u003e=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","nx-cugraph-cu12 24.10.0 requires networkx\u003e=3.0, but you have networkx 2.8.8 which is incompatible.\n","plotnine 0.14.1 requires pandas\u003e=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","xarray 2024.10.0 requires numpy\u003e=1.24, but you have numpy 1.23.5 which is incompatible.\n","xarray 2024.10.0 requires pandas\u003e=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.5\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"f6bb16ff01f5414c8cf465f99c924a46","pip_warning":{"packages":["numpy"]}}},"metadata":{},"output_type":"display_data"}],"source":["! pip install transformers torch speechrecognition\n","! pip install TTS torch\n","! pip install numpy==1.23.5"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":21991,"status":"ok","timestamp":1733310511033,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"VKQaLu8cnWrm"},"outputs":[],"source":["import numpy as np\n","import speech_recognition as sr\n","import torch\n","from TTS.api import TTS\n","import time\n","import os\n","import shutil"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9540,"status":"ok","timestamp":1733310520570,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"vgA1l7-WrbsR","outputId":"a0213f1b-9fa1-4438-98ee-cb4b005376bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n","Requirement already satisfied: anyio\u003c5,\u003e=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro\u003c2,\u003e=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx\u003c1,\u003e=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n","Requirement already satisfied: jiter\u003c1,\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n","Requirement already satisfied: pydantic\u003c3,\u003e=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm\u003e4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n","Requirement already satisfied: typing-extensions\u003c5,\u003e=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna\u003e=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.5.0-\u003eopenai) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.5.0-\u003eopenai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx\u003c1,\u003e=0.23.0-\u003eopenai) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx\u003c1,\u003e=0.23.0-\u003eopenai) (1.0.7)\n","Requirement already satisfied: h11\u003c0.15,\u003e=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*-\u003ehttpx\u003c1,\u003e=0.23.0-\u003eopenai) (0.14.0)\n","Requirement already satisfied: annotated-types\u003e=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1.9.0-\u003eopenai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1.9.0-\u003eopenai) (2.23.4)\n"]}],"source":["# Install the openai package\n","%pip install openai\n","\n","from openai import OpenAI\n","api_key = 'sk-IE1KpyGZOJ6NBbwOkP1iPiIeyQVFj336gPP5DBC9XzIcGbfn'\n","client = OpenAI(api_key=api_key)\n","\n","client = OpenAI(\n","    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n","    api_key=\"sk-IE1KpyGZOJ6NBbwOkP1iPiIeyQVFj336gPP5DBC9XzIcGbfn\",\n","    base_url=\"https://api.chatanywhere.org\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"Y2W7cTqsmLdX"},"source":["Test script"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10137,"status":"ok","timestamp":1733310530705,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"8TNxHnFimDWU","outputId":"d2a80215-69b2-432a-d145-e14793ae9f14"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded model from disk\n"]}],"source":["from tensorflow.keras.models import Sequential, model_from_json\n","\n","json_path = \"/content/drive/MyDrive/ML Team 35/Codes/Chatbot/CNN_model.json\" #google drive path\n","weights_path = \"/content/drive/MyDrive/ML Team 35/Codes/Chatbot/CNN_model_weights.h5\" #google drive path\n","\n","with open(json_path, 'r') as json_file:\n","    loaded_model_json = json_file.read()\n","loaded_model = model_from_json(loaded_model_json)\n","\n","# load weights into new model\n","loaded_model.load_weights(weights_path)\n","\n","print(\"Loaded model from disk\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1748,"status":"ok","timestamp":1733310532449,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"hP5ssAzfmOn_","outputId":"107cf4c6-e310-49eb-97ff-40fb7eabc1fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done\n"]}],"source":["import pickle\n","\n","scaler_path = '/content/drive/MyDrive/ML Team 35/Codes/Chatbot/scaler.pickle'\n","encoder_path = '/content/drive/MyDrive/ML Team 35/Codes/Chatbot/encoder.pickle'\n","\n","with open(scaler_path, 'rb') as f:\n","    scaler2 = pickle.load(f)\n","\n","with open(encoder_path, 'rb') as f:\n","    encoder2 = pickle.load(f)\n","\n","print(\"Done\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1733310532450,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"1EiVhrHemQr4"},"outputs":[],"source":["import librosa\n","def zcr(data,frame_length,hop_length):\n","    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n","    return np.squeeze(zcr)\n","def rmse(data,frame_length=2048,hop_length=512):\n","    rmse=librosa.feature.rms(y=data,frame_length=frame_length,hop_length=hop_length)\n","    return np.squeeze(rmse)\n","def mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n","    mfcc=librosa.feature.mfcc(y=data,sr=sr)\n","    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n","\n","def extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n","    result=np.array([])\n","\n","    result=np.hstack((result,\n","                      zcr(data,frame_length,hop_length),\n","                      rmse(data,frame_length,hop_length),\n","                      mfcc(data,sr,frame_length,hop_length)\n","                     ))\n","    return result"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1733310532450,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"UsV5DKg4mSYF"},"outputs":[],"source":["def get_predict_feat(path):\n","    d, s_rate= librosa.load(path, duration=2.5, offset=0.6)\n","    res=extract_features(d)\n","    result=np.array(res)\n","    result=np.reshape(result,newshape=(1,2376))\n","    i_result = scaler2.transform(result)\n","    final_result=np.expand_dims(i_result, axis=2)\n","\n","    return final_result"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7288,"status":"ok","timestamp":1733310539732,"user":{"displayName":"戴凱麗","userId":"07155429836939668863"},"user_tz":-480},"id":"4nyIGGjSmUIG","outputId":"794cefad-ecb0-4324-d691-a7a58d783e3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"]}],"source":["# all imports\n","from IPython.display import Javascript\n","from google.colab import output\n","from base64 import b64decode\n","from io import BytesIO\n","!pip -q install pydub\n","!apt-get install -y ffmpeg\n","from pydub import AudioSegment\n","\n","RECORD = \"\"\"\n","const sleep  = time =\u003e new Promise(resolve =\u003e setTimeout(resolve, time))\n","const b2text = blob =\u003e new Promise(resolve =\u003e {\n","  const reader = new FileReader()\n","  reader.onloadend = e =\u003e resolve(e.srcElement.result)\n","  reader.readAsDataURL(blob)\n","})\n","var record = time =\u003e new Promise(async resolve =\u003e {\n","  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n","  recorder = new MediaRecorder(stream)\n","  chunks = []\n","  recorder.ondataavailable = e =\u003e chunks.push(e.data)\n","  recorder.start()\n","  await sleep(time)\n","  recorder.onstop = async ()=\u003e{\n","    blob = new Blob(chunks)\n","    text = await b2text(blob)\n","    resolve(text)\n","  }\n","  recorder.stop()\n","})\n","\"\"\"\n","\n","def record(sec=3):\n","  display(Javascript(RECORD))\n","  s = output.eval_js('record(%d)' % (sec*1000))\n","  b = b64decode(s.split(',')[1])\n","  audio = AudioSegment.from_file(BytesIO(b))\n","  return audio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":17},"id":"joDoqf8jmUwu"},"outputs":[{"data":{"application/javascript":["\n","const sleep  = time =\u003e new Promise(resolve =\u003e setTimeout(resolve, time))\n","const b2text = blob =\u003e new Promise(resolve =\u003e {\n","  const reader = new FileReader()\n","  reader.onloadend = e =\u003e resolve(e.srcElement.result)\n","  reader.readAsDataURL(blob)\n","})\n","var record = time =\u003e new Promise(async resolve =\u003e {\n","  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n","  recorder = new MediaRecorder(stream)\n","  chunks = []\n","  recorder.ondataavailable = e =\u003e chunks.push(e.data)\n","  recorder.start()\n","  await sleep(time)\n","  recorder.onstop = async ()=\u003e{\n","    blob = new Blob(chunks)\n","    text = await b2text(blob)\n","    resolve(text)\n","  }\n","  recorder.stop()\n","})\n"],"text/plain":["\u003cIPython.core.display.Javascript object\u003e"]},"metadata":{},"output_type":"display_data"},{"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'split'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-8-48559f959b75\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maudio_path_colab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/recorded_audio.wav\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path_colab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Audio saved to Colab local storage: {audio_path_colab}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-7-e5459fb52ae1\u003e\u001b[0m in \u001b[0;36mrecord\u001b[0;34m(sec)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJavascript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRECORD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'record(%d)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 36\u001b[0;31m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"]}],"source":["audio = record(sec=5)\n","\n","audio_path_colab = \"/content/recorded_audio.wav\"\n","audio.export(audio_path_colab, format=\"wav\")\n","print(f\"Audio saved to Colab local storage: {audio_path_colab}\")\n","\n","audio_path_drive = \"/content/drive/MyDrive/ML Team 35/Codes/Chatbot/recorded_audio.wav\"\n","audio.export(audio_path_drive, format=\"wav\")\n","print(f\"Audio also saved to Google Drive: {audio_path_drive}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y7h4VMvimXUF"},"outputs":[],"source":["from IPython.display import Audio\n","Audio(audio_path_drive)\n","Audio(audio_path_colab)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EcaMXs1bmhvP"},"outputs":[],"source":["emotions1={1:'Neutral', 2:'Calm', 3:'Happy', 4:'Sad', 5:'Angry', 6:'Fearful', 7:'Disgust',8:'Surprised'}\n","def prediction(path1):\n","    res=get_predict_feat(path1) #提取特征\n","    predictions=loaded_model.predict(res,verbose=0) #模型預測\n","    y_pred = encoder2.inverse_transform(predictions) #解碼預測結果\n","    return (y_pred[0][0])  #輸出情感類別"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jv0IIU6Omi8f"},"outputs":[],"source":["print(prediction(audio_path_colab))"]},{"cell_type":"markdown","metadata":{"id":"QeWEBcDsL2Zn"},"source":["Language detection"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Zy9w6IYwL494"},"outputs":[],"source":["!pip install git+https://github.com/openai/whisper.git\n","!sudo apt update \u0026\u0026 sudo apt install ffmpeg\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"d9uANeOAL_QG"},"outputs":[],"source":["import whisper\n","\n","# 加载模型\n","model = whisper.load_model(\"base\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pSIBwkbYSuwI"},"outputs":[],"source":["# 加载音频文件\n","result = model.transcribe(audio_path_colab, task=\"language\")  # task=\"language\"用于检测语言\n","print(\"Detected Language:\", result['language'])\n","speaker_language = result['language']\n","if speaker_language == \"zh\" :\n","  speaker_language = \"zh-cn\""]},{"cell_type":"markdown","metadata":{"id":"9nOubKGoq80Q"},"source":["STT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TXqj-fgXq-8i"},"outputs":[],"source":["def speech_to_text(audio_file):\n","    recognizer = sr.Recognizer()\n","    with sr.AudioFile(audio_file) as source:\n","        audio_data = recognizer.record(source)\n","    try:\n","        text = recognizer.recognize_google(audio_data, language=speaker_language)\n","        return text\n","    except sr.UnknownValueError:\n","        return \"Sorry, I couldn't understand the audio.\"\n","    except sr.RequestError:\n","        return \"Sorry, there was a request error.\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8nV1Zo8HrVmN"},"outputs":[],"source":["stt_result = speech_to_text(audio_path_colab)\n","print(stt_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LPL_zJZPrEUC"},"outputs":[],"source":["def get_ser_label(audio_file):\n","    ser_result = prediction(audio_file)  # Replace with dynamic emotion inference in actual usage\n","    return ser_result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"emGBCeJTroF-"},"outputs":[],"source":["speech_emotion = get_ser_label(audio_path_colab)\n","print(speech_emotion)"]},{"cell_type":"markdown","metadata":{"id":"1dppoJsjqs_0"},"source":["GPT 3.5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pYcynSwcjx1m"},"outputs":[],"source":["def gpt_35_api(messages: list, language: str = \"en\", emotion: str = None):\n","    \"\"\"为提供的对话消息创建新的回答，根据传入的语言参数和情绪决定回复。\n","\n","    Args:\n","        messages (list): 完整的对话消息。\n","        language (str): 回复语言代码（如 \"en\", \"zh\", \"es\"）。\n","        emotion (str): 识别的用户情绪（如 \"happy\", \"sad\", \"angry\"）。\n","    \"\"\"\n","    # 根据传入语言参数设置系统消息\n","    language_context = {\n","        \"en\": \"You are an assistant that replies in English. Please respond with empathy considering the user's emotion.\",\n","        \"es\": \"Eres un asistente que responde en español. Responde con empatía considerando la emoción del usuario.\",\n","        \"fr\": \"Vous êtes un assistant qui répond en français. Répondez avec empathie en tenant compte de l'émotion de l'utilisateur.\",\n","        \"de\": \"Du bist ein Assistent, der auf Deutsch antwortet. Bitte antworte einfühlsam und berücksichtige die Emotion des Benutzers.\",\n","        \"it\": \"Sei un assistente che risponde en italiano. Rispondi con empatia considerando l'emozione dell'utente.\",\n","        \"pt\": \"Você é um assistente que responde em português. Responda com empatia considerando a emoção do usuário.\",\n","        \"pl\": \"Jesteś asystentem, który odpowiada po polsku. Odpowiedz z empatią, biorąc pod uwagę emocje użytkownika.\",\n","        \"tr\": \"Türkçe cevap veren bir asistansın. Kullanıcının duygusunu dikkate alarak empatiyle yanıt ver.\",\n","        \"ru\": \"Вы — помощник, отвечающий на русском языке. Пожалуйста, ответьте с сочувствием, учитывая эмоции пользователя.\",\n","        \"nl\": \"Je bent een assistent die in het Nederlands antwoordt. Reageer met empathie, rekening houdend met de emoties van de gebruiker.\",\n","        \"cs\": \"Jste asistent, který odpovídá v češtině. Odpovězte s empatií s ohledem na emoce uživatele.\",\n","        \"ar\": \"أنت مساعد يجيب باللغة العربية. يرجى الرد بتعاطف مع مراعاة مشاعر المستخدم.\",\n","        \"zh-cn\": \"你是一个使用中文回复的助手。请根据用户的情绪提供有同理心的回复。\",\n","        \"ja\": \"日本語で返信するアシスタントです。ユーザーの感情を考慮して共感的に返信してください。\",\n","        \"hu\": \"Ön egy asszisztens, aki magyarul válaszol. Kérem, válaszoljon empatikusan, figyelembe véve a felhasználó érzelmeit.\",\n","        \"ko\": \"한국어로 응답하는 어시스턴트입니다. 사용자의 감정을 고려하여 공감으로 응답하세요。\",\n","        \"hi\": \"आप एक सहायक हैं जो हिंदी में जवाब देता है। कृपया उपयोगकर्ता की भावना को ध्यान में रखते हुए सहानुभूति के साथ जवाब दें।\"\n","    }\n","\n","    system_message = language_context.get(language, \"You are an assistant that replies in English. Please respond with empathy.\")\n","\n","    # 将情绪注入到上下文\n","    if emotion:\n","        system_message += f\" The user's detected emotion is '{emotion}'. Please adjust your response accordingly.\"\n","\n","    # 插入系统消息\n","    messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n","\n","    # 调用 GPT API\n","    completion = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\", messages=messages)\n","    reply = completion.choices[0].message.content\n","    return reply\n","\n","\n","def gpt_35_api_stream(messages: list, language: str = \"en\", emotion: str = None):\n","    \"\"\"为提供的对话消息创建新的回答 (流式传输)，根据传入的语言参数和情绪决定回复。\n","\n","    Args:\n","        messages (list): 完整的对话消息。\n","        language (str): 回复语言代码（如 \"en\", \"zh\", \"es\"）。\n","        emotion (str): 识别的用户情绪（如 \"happy\", \"sad\", \"angry\"）。\n","    \"\"\"\n","    # 根据传入语言参数设置系统消息\n","    language_context = {\n","        \"en\": f\"You are an assistant that replies in English. Please respond with empathy considering the user's emotion.\",\n","        \"zh-cn\": f\"你是一个使用中文回复的助手。请根据用户的情绪提供有同理心的回复。\",\n","        \"es\": f\"Eres un asistente que responde en español. Responde con empatía considerando la emoción del usuario.\",\n","        \"fr\": f\"Vous êtes un assistant qui répond en français. Répondez avec empathie en tenant compte de l'émotion de l'utilisateur.\",\n","        \"de\": f\"Du bist ein Assistent, der auf Deutsch antwortet. Bitte antworte einfühlsam und berücksichtige die Emotion des Benutzers.\",\n","        \"ja\": f\"日本語で返信するアシスタントです。ユーザーの感情を考慮して共感的に返信してください。\",\n","        \"ko\": f\"한국어로 응답하는 어시스턴트입니다. 사용자 감정을 고려하여 공감으로 응답하십시오。\",\n","        \"hi\": f\"आप एक सहायक हैं जो हिंदी में जवाब देता है। कृपया उपयोगकर्ता की भावना को ध्यान में रखते हुए सहानुभूति के साथ जवाब दें।\"\n","    }\n","    system_message = language_context.get(language, \"You are an assistant that replies in English. Please respond with empathy.\")\n","\n","    # 将情绪注入到上下文\n","    if emotion:\n","        system_message += f\" The user's detected emotion is '{emotion}'. Please adjust your response accordingly.\"\n","\n","    # 插入系统消息\n","    messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n","\n","    # 调用 GPT API (流式)\n","    reply = \"\"\n","    stream = client.chat.completions.create(\n","        model='gpt-3.5-turbo',\n","        messages=messages,\n","        stream=True,\n","    )\n","    for chunk in stream:\n","        if chunk.choices[0].delta.content is not None:\n","            content = chunk.choices[0].delta.content\n","            reply += content\n","    return reply\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"04nic7_CCJcb"},"outputs":[],"source":["def gpt_35_api_stream(messages: list, language: str = \"en\", emotion: str = None):\n","    \"\"\"为提供的对话消息创建新的回答 (流式传输)，根据传入的语言参数和情绪决定回复。\n","\n","    Args:\n","        messages (list): 完整的对话消息。\n","        language (str): 回复语言代码（如 \"en\", \"zh\", \"es\"）。\n","        emotion (str): 识别的用户情绪（如 \"Happy\", \"Sad\", \"Angry\"）。\n","    \"\"\"\n","    # 根据传入语言参数设置系统消息\n","    language_context = {\n","        \"en\": f\"You are an assistant that replies in English. Please respond with empathy considering the user's emotion.\",\n","        \"zh-cn\": f\"你是一个使用中文回复的助手。请根据用户的情绪提供有同理心的回复。\",\n","        \"es\": f\"Eres un asistente que responde en español. Responde con empatía considerando la emoción del usuario.\",\n","        \"fr\": f\"Vous êtes un assistant qui répond en français. Répondez avec empathie en tenant compte de l'émotion de l'utilisateur.\",\n","        \"de\": f\"Du bist ein Assistent, der auf Deutsch antwortet. Bitte antworte einfühlsam und berücksichtige die Emotion des Benutzers.\",\n","        \"ja\": f\"日本語で返信するアシスタントです。ユーザーの感情を考慮して共感的に返信してください。\",\n","        \"ko\": f\"한국어로 응답하는 어시스턴트입니다. 사용자 감정을 고려하여 공감으로 응답하십시오。\",\n","        \"hi\": f\"आप एक सहायक हैं जो हिंदी में जवाब देता है। कृपया उपयोगकर्ता की भावना को ध्यान में रखते हुए सहानुभूति के साथ जवाब दें।\"\n","    }\n","    system_message = language_context.get(language, \"You are an assistant that replies in English. Please respond with empathy.\")\n","\n","    # 将情绪注入到上下文并增强逻辑\n","    if emotion:\n","        emotion_guidelines = {\n","            \"Neutral\": \"Respond normally based on the user's content.\",\n","            \"Calm\": \"Respond in a composed and supportive manner.\",\n","            \"Happy\": \"Share their joy and respond with an uplifting and enthusiastic tone.\",\n","            \"Sad\": \"Provide comforting and encouraging words to uplift their mood.\",\n","            \"Angry\": \"Help them calm down and encourage a rational perspective. Avoid escalating the situation.\",\n","            \"Fearful\": \"Offer reassurance and words of encouragement to help them overcome their fears.\",\n","            \"Disgust\": \"Address their concerns respectfully and provide supportive and understanding feedback.\",\n","            \"Surprised\": \"Encourage them to share more details and join in their excitement.\"\n","        }\n","        # 根据情绪类型调整系统消息\n","        emotion_instruction = emotion_guidelines.get(emotion, \"Respond with empathy considering the user's emotion.\")\n","        system_message += f\" The user's detected emotion is '{emotion}'. {emotion_instruction}\"\n","\n","    # 插入系统消息\n","    messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n","\n","    # 调用 GPT API (流式)\n","    reply = \"\"\n","    stream = client.chat.completions.create(\n","        model='gpt-3.5-turbo',\n","        messages=messages,\n","        stream=True,\n","    )\n","    for chunk in stream:\n","        if chunk.choices[0].delta.content is not None:\n","            content = chunk.choices[0].delta.content\n","            reply += content\n","    return reply\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iWXEotvLkvYB"},"outputs":[],"source":["# 准备对话消息\n","summarizing_factor = \"(reply in one sentence with more concerning with speaker's emotion)\"\n","messages = [{'role': 'user', 'content': stt_result + summarizing_factor + speech_emotion}]\n","\n","# 调用 GPT 流式 API\n","gpt_reply = gpt_35_api_stream(messages, language=speaker_language, emotion=speech_emotion)\n","print(gpt_reply, end=\"\")"]},{"cell_type":"markdown","metadata":{"id":"szk8y_hj1QXA"},"source":["TTS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VUqtE05LJ-pn"},"outputs":[],"source":["# 检查设备是否支持 GPU\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Running on device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9R_eh9zu1RY3"},"outputs":[],"source":["model_name = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n","\n","tts = TTS(model_name).to(device)\n","print(f\"Model {model_name} is loaded and ready.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jg4-ZgAJ2bHD"},"outputs":[],"source":["TEXT = gpt_reply\n","EMOTION = speech_emotion  # \"Happy\", \"Sad\"\n","LANGUAGE = speaker_language  # \"zh-cn\"，\"en\"\n","\n","output_dir_colab = \"./output/\"\n","os.makedirs(output_dir_colab, exist_ok=True)\n","output_file_path_colab = os.path.join(output_dir_colab, f\"output_{EMOTION}.wav\")\n","\n","output_dir_drive = \"/content/drive/MyDrive/ML Team 35/Codes/Chatbot/Generated_Audio/\"\n","os.makedirs(output_dir_drive, exist_ok=True)\n","output_file_path_drive = os.path.join(output_dir_drive, f\"output_{EMOTION}.wav\")\n","\n","start = time.perf_counter()\n","tts.tts_to_file(\n","    text=TEXT,\n","    speaker_wav=\"/content/drive/MyDrive/ML Team 35/Codes/Chatbot/LiShen.wav\",\n","    language=LANGUAGE,\n","    file_path=output_file_path_colab,\n","    emotion=EMOTION,\n",")\n","end = time.perf_counter()\n","\n","shutil.copy(output_file_path_colab, output_file_path_drive)\n","\n","print(f\"Audio file saved to Colab: {output_file_path_colab}\")\n","print(f\"Audio file also saved to Drive: {output_file_path_drive}\")\n","print(f\"Generation time: {end - start:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Hy1xkdfh8QMO"},"outputs":[],"source":["from IPython.display import Audio\n","\n","if os.path.exists(output_file_path_colab):\n","    print(f\"Playing generated file: {output_file_path_colab}\")\n","    display(Audio(output_file_path_colab))\n","else:\n","    print(\"Generated file not found. Please check the paths and settings.\")\n"]}],"metadata":{"colab":{"name":"","provenance":[{"file_id":"1brk8gQen5aGoZA7aO9deNFGXubHHlVXr","timestamp":1732560373443}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}